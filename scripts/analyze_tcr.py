#!/usr/bin/env python"""Description: There are five steps in TCR analyzing pipeline  Step 1: Map FASTQ file to well  Step 2: Generate BLAST wrapper  Step 3: Count and seperatevreads aligned to TRA, TRB and target genes  Step 4: Generate MiXCR wrapper  Step 5: Get clonotype per cellUsage: python analyze_tcr.pyJing SunSep, 2017"""import osimport sysimport argparseimport reimport csvimport pandasimport timefrom collections import defaultdictfrom collections import Counterimport gzipfrom Bio import pairwise2from Bio.pairwise2 import format_alignmentfrom config import *BLAST_DATABASE_DIR = ROOT_DIR + "blast_database/"TARGET_GENE_LIST_PATH = BLAST_DATABASE_DIR + "target_gene_map_name_to_primers.txt"SRC_DIR = ROOT_DIR + "scripts/"COLLAPSE_RULES_PATH = SRC_DIR + "collapse_rules.txt"nested_dict = lambda: defaultdict(nested_dict)today = time.strftime("%Y%m%d")step_description = {"map": "Step 1: Map FASTQ file to well",                    "blast": "Step 2.1: Generate BLAST wrapper",                    "blast_parse": "Step 2.2: Generate BLAST parse wrapper",                    "blast_count": "Step 2.3: Count BLAST results aligned to TRA, TRB and target genes",                    "separate": "Step 3: Separate FASTQ files to TRA and TRB",                    "mixcr": "Step 4.1: Generate MiXCR wrapper",                    "merge_TRBV": "Step 4.15: Merge indistinguishable V segments in clone.txt files",                    "mixcr_umi": "Step 4.2: Generate UMI count wrapper",                    "mixcr_parse": "Step 4.3: Get clonotype per cell",                    "master": "One script for all."                    }# default parameters## mainly for sctcr dataCLONE_COUNT_CUTOFF = 5CLONE_PERC_SUM_CUTOFF = 90collapse_identity = 0.95def main():    parser = argparse.ArgumentParser(description="Analyze TCR data")    parser.add_argument("--info",                        dest="run_info_path",                        default=None,                        type=str,                        help="file of run information")    parser.add_argument("--step",                        dest="step",                        default=None,                        type=str,                        help="step to run")    args = parser.parse_args()    run_info_path = args.run_info_path    if run_info_path == None:        run_info_path = ROOT_DIR + "out" + "/" + RUN_NAME + "/run_info.csv"    step = args.step    print("\n" +          "#" + "-" * 50 + "\n\n" +          step_description[step] + "\n\n")    # read run info    run_list = []    with open(run_info_path) as run_info_file:        for row in csv.DictReader(run_info_file, skipinitialspace = True):            run_list = [{k: v for k, v in row.items()}]    run_info_file.close()    print("run_list: ", run_list)    # process each run    for run in run_list:        run_name = run["run_name"]        index_path = run["index_path"]        target_gene_exist = run["target_gene"]        umi_exist = run["umi"]        sc_data = run["single"]        if "other" in run:            other_method = run["other"]        else:            other_method = "N"                    fastq_dir = run["fastq_dir"]        run_dir = run["run_dir"]        print("          [run name]  " + run_name + "\n" +              "        [index path]  " + index_path + "\n" +              " [target gene exist]  " + target_gene_exist + "\n" +              "         [umi exist]  " + umi_exist + "\n" +              "       [single-cell]  " + sc_data + "\n" +              "        [input path]  " + fastq_dir + "\n" +              "       [output path]  " + run_dir + "\n")        os.makedirs(os.path.join(run_dir, "results"), exist_ok = True)                if step == "map":            mapFastq2Well(index_path, fastq_dir, run_dir)        # read fastq mapping table        fastq_dict = nested_dict()        fastq_list = []            map_path = os.path.join(run_dir, "results", "fastq_basename2well.list")        try:            well_row_set = set()            well_col_set = set()            well2fastq_basename = {}                with open(map_path) as map_file:                map_data = pandas.read_table(map_file, sep = "\t", header = 0)                for index, row in map_data.iterrows():                    well = row["well"]                    well_row_set.add(well[0:1])                    well_col_set.add(well[1:])                    well2fastq_basename[well] = row["fastq_basename"]                        fastq_dict[row["fastq_basename"]] = {"well": well,                                                        "read_count": row["read_count"]}            map_file.close()                for well_row in sorted(well_row_set):                for well_col in sorted(well_col_set, key = int):                    if well_row + well_col in well2fastq_basename:                        fastq_list.append(well2fastq_basename[well_row + well_col])            except:            pass        fastq_list_path = os.path.join(run_dir, "results", "fastq_basename4blast.list")                if step == "master":            generateMaster(run_name, fastq_dir, run_dir, target_gene_exist, umi_exist, sc_data, run_info_path)        if step == "blast":            generateWrapper("blast", run_name, fastq_dir, run_dir, target_gene_exist, umi_exist, fastq_list_path)        if step == "blast_parse":            generateWrapper("blast_parse", run_name, fastq_dir, run_dir, target_gene_exist, umi_exist, fastq_list_path)        if step == "blast_count":            countBlastResults(run_dir, fastq_list, fastq_dict, target_gene_exist)        if step == "separate":            generateWrapper("separate", run_name, fastq_dir, run_dir, target_gene_exist, umi_exist, fastq_list_path)        fastq_list_path = os.path.join(run_dir, "results", "fastq_basename4mixcr.list")                if other_method == "Y":            if step == "mixcr":                # Make index file for running mixcr                with open(fastq_list_path, "w") as fastq_list_file:                    for fastq_basename in fastq_list:                        for loci in ["TRA", "TRB"]:                            if fastq_dict[fastq_basename]["read_count"] > 0:                                fastq_list_file.write("\t".join([fastq_basename, loci]) + "\n")                generateWrapper("mixcr_other", run_name, fastq_dir, run_dir, target_gene_exist, umi_exist, fastq_list_path)        else:            if step == "mixcr":                generateWrapper("mixcr", run_name, os.path.join(run_dir, "fastq", "tcr"), run_dir, target_gene_exist, umi_exist, fastq_list_path)                            if step == "merge_TRBV":                generateWrapper("merge_TRBV", run_name, os.path.join(run_dir, "fastq", "tcr"), run_dir, target_gene_exist, umi_exist, fastq_list_path)            if step == "mixcr_umi":                generateWrapper("mixcr_umi", run_name, os.path.join(run_dir, "fastq", "tcr"), run_dir, target_gene_exist, umi_exist, fastq_list_path)            if step == "mixcr_parse":            parseMixcrResults(run_dir, fastq_list, fastq_dict, umi_exist)        print("#" + "-" * 50 + "\n")def MapOther():        # get list of fastq files    fastq_list = []    for fastq_file in os.listdir(fastq_dir):        s = re.search("(.+)_(.+)_L001_R1_001.fastq.gz", fastq_file)            if not s:  # skip if r2/i1/i2 fastq files            continue            if os.stat(os.path.join(fastq_dir, fastq_file)).st_size == 0 or os.stat(os.path.join(fastq_dir, fastq_file)).st_size <= 560:  # skip if empty file            continue            fastq_basename = s.group(1) + "_" + s.group(2)        if fastq_basename != "Undetermined_S0":            fastq_list.append(fastq_basename)            read_count[fastq_basename] = countRead(fastq_file, "fastq")    # print fastq mapping files    map_path = os.path.join(run_dir, "results", "fastq_basename2well.list")    with open(map_path, "w") as map_file:        map_file.write("\t".join(["fastq_basename", "well", "read_count"]) + "\n")        map_file.write("\t".join([well2fastq_basename[well], well, str(well2read_count[well])]) + "\n")    map_file.close()def mapRead2UMI(fastq_dir, run_dir):    os.makedirs(os.path.join(fastq_dir, "umi_collapsed"), exist_ok = True)    fastq_basename_list = []    fastq_basename_path = os.path.join(run_dir, "results", "fastq_basename4blast.list")    with open(fastq_basename_path) as fastq_basename_file:        fastq_basename_data = pandas.read_table(fastq_basename_file, sep = "\t", header = None)        fastq_basename_list = fastq_basename_data[0].tolist()    fastq_basename_file.close()    print("length of fastq_basename_list: ", len(fastq_basename_list))    counter = 0    for fastq_basename in fastq_basename_list: # loop through all fastq basenames        if counter < 28:            counter += 1            continue        counter += 1        print("fastq_basename: ", fastq_basename)        umi2seq = {}        seq2id = {}        id2seq = {}        R1_path = os.path.join(fastq_dir, fastq_basename + "_L001_R1_001.fastq.gz")        with gzip.open(R1_path, "rt") as fastq_file:            id = ""            seq = ""            for count, row in enumerate(fastq_file, start = 1):                if count % 4 == 1:                    id = re.search("(@.+?)\s", row).group(1)                if count % 4 == 2:                    seq = row.rstrip()                    seq2id[seq] = id                    id2seq[id] = seq        fastq_file.close()        R2_path = os.path.join(fastq_dir, fastq_basename + "_L001_R2_001.fastq.gz")        with gzip.open(R2_path, "rt") as fastq_file:            id = ""            umi = ""            for count, row in enumerate(fastq_file, start = 1):                if count % 4 == 1:                    id = re.search("(@.+?)\s", row).group(1)                if count % 4 == 2:                    umi = row[0:7]                    seq = id2seq[id]                    if umi not in umi2seq:                        umi2seq[umi] = []                    umi2seq[umi].append(seq)        fastq_file.close()        collapseReads(fastq_basename, umi2seq, seq2id, fastq_dir)def collapseReads(fastq_basename, umi2seq, seq2id, fastq_dir):    saved_reads = [] #list to hold all kept reads across all umis    for umi in umi2seq: #iterate through umis        countSeq = Counter(umi2seq[umi])        maxSeq = countSeq.most_common(1)[0][0]        id = seq2id[maxSeq]        saved_reads.append(id)    writeUmiCollapsed(fastq_basename, fastq_dir, saved_reads)def writeUmiCollapsed(fastq_basename, fastq_dir, saved_reads):    print("writing fastq_basename: ", fastq_basename)    suffix_list = ["_L001_R1_001.fastq.gz", "_L001_R2_001.fastq.gz", "_L001_I1_001.fastq.gz", "_L001_I2_001.fastq.gz"]    for suffix in suffix_list:        in_path = os.path.join(fastq_dir, fastq_basename + suffix)        out_path = os.path.join(fastq_dir, "umi_collapsed", fastq_basename + suffix)        with gzip.open(in_path, 'rt') as in_file:            with gzip.open(out_path, 'wt') as out_file:                keep = False                for count, row in enumerate(in_file, start = 1):                    if count % 4 == 1:                        read = row.split()[0]                        if read in saved_reads:                            keep = True                            out_file.write(row)                        else:                            keep = False                    else:                        if keep:                            out_file.write(row)            out_file.close()        in_file.close()def mapFastq2Well(index_path, fastq_dir, run_dir):    # read index to plate layout    index_dict = nested_dict()    well_list = []    index2well = nested_dict()    with open(index_path) as index_file:        index_data = pandas.read_table(index_file, sep = "\t", header = 0)        for index, row in index_data.iterrows():            index1 = row["Index1"].upper()            index2 = row["Index2"].upper()            well = row["well"]            index_dict["I1"][index1] = ""            index_dict["I2"][index2] = ""            well_list.append(well)            index2well[index1][index2] = well    index_file.close()    well_row_set = set()    well_col_set = set()    for well in well_list:        well_row_set.add(well[0:1])        well_col_set.add(well[1:])    # map fastq to plate layout    well2read_count = {}    well2fastq_basename = {}    for fastq_file in os.listdir(fastq_dir):        s = re.search("(.+)_(.+)_L001_R1_001.fastq.gz", fastq_file)        if not s:  # skip if r2/i1/i2 fastq files            continue        if os.stat(os.path.join(fastq_dir, fastq_file)).st_size == 0 or os.stat(os.path.join(fastq_dir, fastq_file)).st_size <= 560:  # skip if empty file            print("empty fastq file: ", fastq_file)            continue        fastq_basename = s.group(1) + "_" + s.group(2)        if fastq_basename != "Undetermined_S0":            index1 = findIndex(os.path.join(fastq_dir, fastq_basename + "_L001_I1_001.fastq.gz"), index_dict["I1"], index_file)            index2 = findIndex(os.path.join(fastq_dir, fastq_basename + "_L001_I2_001.fastq.gz"), index_dict["I2"], index_file)            well = index2well[index1][index2]  # well id from index combination        else:            well = "Undetermined"        fastq_path = os.path.join(fastq_dir, fastq_basename + "_L001_I1_001.fastq.gz")        well2read_count[well] = countRead(fastq_path, "fastq")        well2fastq_basename[well] = fastq_basename    # print list of fastq files    fastq_list_path = os.path.join(run_dir, "results", "fastq_basename4blast.list")    with open(fastq_list_path, "w") as fastq_list_file:        for well in well_list:            if well in well2read_count:                fastq_list_file.write(well2fastq_basename[well] + "\n")    fastq_list_file.close()    # print fastq mapping files    map_path = os.path.join(run_dir, "results", "fastq_basename2well.list")    with open(map_path, "w") as map_file:        map_file.write("\t".join(["fastq_basename", "well", "read_count"]) + "\n")        for well in well_list:            if well in well2read_count:                map_file.write("\t".join([well2fastq_basename[well], well, str(well2read_count[well])]) + "\n")    map_file.close()def countRead(read_file_path, file_type):    if not os.path.exists(read_file_path):        print("Error: file does not exist, %s" % read_file_path)        sys.exit(1)        count = 0    with gzip.open(read_file_path, "rt") as read_file:        for line in read_file:            count += 1    read_file.close()    if file_type == "fastq":        count /= 4    elif file_type == "fasta":        count /= 2    if int(count) != count:        print("Error: read count is not integer, %s " % (count, read_file_path))        sys.exit(1)    return int(count)def findIndex(fastq_path, index_dict, index_file):    index_found = ""    with gzip.open(fastq_path, "rt") as fastq_file:        for count, row in enumerate(fastq_file, start = 1):            if count % 4 == 2:                index = row.strip()                if index in index_dict:                    index_found = index                else:                    for index_in_dict in index_dict:                        if compareIndex(index, index_in_dict, 1):                            index_found = index_in_dict                            break                if index_found != "":                    break            count += 1    fastq_file.close()        if index_found == "":        print("Index not from the plate [%s]: %s" %(index_file, fastq_path))        sys.exit(1)    return index_founddef compareIndex(i1, i2, max):    compare = True    n = 0    for c1, c2 in zip(i1, i2):        if c1 != c2:            n += 1            if n > max:                compare = False                break    return comparedef generateMaster(run_name, fastq_dir, run_dir, target_gene_exist, umi_exist, sc_data, run_info_path):    current_time = time.strftime("%Y-%m-%d-%H.%M")    blast_fastq_list_path = os.path.join(run_dir, "results", "fastq_basename4blast.list")    mixcr_fastq_list_path = os.path.join(run_dir, "results", "fastq_basename4mixcr.list")    # generate wrapper to run scripts    program_list = ['blast', 'blast_parse', 'separate', 'mixcr', 'merge_TRBV', 'mixcr_umi', 'master']    log_dirs = {}    for program in program_list:        log_dirs[program] = os.path.join(run_dir, "logs", program)        os.makedirs(log_dirs[program], exist_ok=True)    master_path = os.path.join(log_dirs['master'], 'master.sh')    with open(master_path, "w") as master:        master.write("#!/bin/bash" + "\n" +                           "#SBATCH -c 20 #Number of cores\n" +                           "#SBATCH -N 1\n" +                           "#SBATCH -t 15:00:00 # Runtime in minutes\n" +                           "#SBATCH --mem=20000 #Memory per node in MB (see also --mem-per-cpu)\n" +                           "#SBATCH -p medium\n" +                           "#SBATCH -o " + run_dir + '/logs/master/master.out' + "\n" +                           "#SBATCH -J " + 'master' + "_" + run_name + "\n" +                           "#SBATCH --mail-type=ALL" + '\n' +                           "#SBATCH --mail-user=jforman@broadinstitute.org"                           "\n" +                           "### define parameters" + "\n" +                           "\n" +                           "# batch job parameters" + "\n" +                           "source `which env_parallel.bash`\n"                           "env_parallel --citation 'will cite'" + "\n" +                           'echo ____________________\necho\necho start setup' + '\n' +                           "BLAST_DATABASE_DIR=" + BLAST_DATABASE_DIR + "\n" +                           "MIXCR_JAR=" + MIXCR_JAR + "\n" +                           "fastq_dir=" + fastq_dir + "/" + "\n" +                           "mixcr_fastq_dir=" + run_dir + '/fastq/tcr/' + "\n" +                           "run_dir=" + run_dir + "/" + "\n" +                           "target_gene_exist=" + target_gene_exist + "\n" +                           "umi_exist=" + umi_exist + "\n" +                           "sc_data=" + sc_data + "\n" +                           "run_info=" + run_info_path + "\n" +                           ">${run_dir}logs/master/master.out" + '\n' +                           "\n" +                           "# sample arguments" + "\n" +                           "blast_fastq_list_path=" + blast_fastq_list_path + "\n" +                           "mixcr_fastq_list_path=" + mixcr_fastq_list_path + "\n" +                           "src_dir=" + SRC_DIR + '\n' +                           'echo setup completed\necho\necho ____________________\necho\necho start map' + '\n' +                           'python3 ${src_dir}analyze_tcr.py --info $run_info --step map' + '\n' +                           'sample_range=`python ${src_dir}get_parallel_range.py --fastq_list_path $blast_fastq_list_path`\n' +                           "blast_log_dir=" + log_dirs['blast'] + "/" + "\n\n" +                           "blast_log_file=${blast_log_dir}array_jobs.log" + "\n" +                           "blast_parse_log_dir=" + log_dirs['blast_parse'] + "/" + "\n\n" +                           "blast_parse_log_file=${blast_parse_log_dir}array_jobs.log" + "\n" +                           "separate_log_dir=" + log_dirs['separate'] + "/" + "\n\n" +                           "separate_log_file=${separate_log_dir}array_jobs.log" + "\n" +                           "mixcr_log_dir=" + log_dirs['mixcr'] + "/" + "\n\n" +                           "mixcr_log_file=${mixcr_log_dir}array_jobs.log" + "\n" +                           "merge_TRBV_log_dir=" + log_dirs['merge_TRBV'] + "/" + "\n\n" +                           "merge_TRBV_log_file=${mixcr_log_dir}array_jobs.log" + "\n" +                           "mixcr_umi_log_dir=" + log_dirs['mixcr_umi'] + "/" + "\n\n" +                           "mixcr_umi_log_file=${mixcr_umi_log_dir}array_jobs.log" + "\n" +                           "if [[ ! -e $blast_log_file ]]; then" + "\n" +                           "    echo -e \"slurm_no\tsample_id\tstart_time\tend_time\telapsed_time\tstatus\" > $blast_log_file" + "\n" +                           "fi" + "\n" +                           "if [[ ! -e $blast_parse_log_file ]]; then" + "\n" +                           "    echo -e \"slurm_no\tsample_id\tstart_time\tend_time\telapsed_time\tstatus\" > $blast_parse_log_file" + "\n" +                           "fi" + "\n" +                           "if [[ ! -e $separate_log_file ]]; then" + "\n" +                           "    echo -e \"slurm_no\tsample_id\tstart_time\tend_time\telapsed_time\tstatus\" > $separate_log_file" + "\n" +                           "fi" + "\n" +                           "if [[ ! -e $mixcr_log_file ]]; then" + "\n" +                           "    echo -e \"slurm_no\tsample_id\tstart_time\tend_time\telapsed_time\tstatus\" > $mixcr_log_file" + "\n" +                           "fi" + "\n" +                           "if [[ ! -e $merge_TRBV_log_file ]]; then" + "\n" +                           "    echo -e \"slurm_no\tsample_id\tstart_time\tend_time\telapsed_time\tstatus\" > $merge_TRBV_log_file" + "\n" +                           "fi" + "\n" +                           "if [[ ! -e $mixcr_umi_log_file ]]; then" + "\n" +                           "    echo -e \"slurm_no\tsample_id\tstart_time\tend_time\telapsed_time\tstatus\" > $mixcr_umi_log_file" + "\n" +                           "fi" + "\n" +                           'echo map completed\necho\necho ____________________\necho\necho start blast' + '\n' +                           'python3 ${src_dir}print_description.py --info $run_info --step blast' + '\n' +                           'env_parallel \'blast_fastq_basename=`awk "NR=={}" $blast_fastq_list_path | awk \'\\\'\'{print $1}\'\\\'\'` && bash ${src_dir}blast.sh "$fastq_dir" "$blast_fastq_basename" "$run_dir" "$target_gene_exist" "$blast_log_file" {} "$BLAST_DATABASE_DIR" &> ${blast_log_dir}${blast_fastq_basename}.out\'' + ' ::: $sample_range' + '\n' +                           'wait\n' +                           'echo blast completed\necho\necho ____________________\necho\necho start blast parse' + '\n' +                           'python3 ${src_dir}print_description.py --info $run_info --step blast_parse' + '\n' +                           'env_parallel \'blast_parse_fastq_basename=`awk "NR=={}" $blast_fastq_list_path | awk \'\\\'\'{print $1}\'\\\'\'` && python3 ${src_dir}parse_blast_results.py --run_dir $run_dir --fastq_basename $blast_parse_fastq_basename --target_gene_exist $target_gene_exist --umi_exist $umi_exist --log_path $blast_parse_log_file --sge_task_id {} &> ${blast_parse_log_dir}${blast_parse_fastq_basename}.out\' ::: $sample_range' + '\n' +                           'wait' + '\n' +                           'echo blast parse completed\necho\necho ____________________\necho\necho start blast count' + '\n' +                           'python3 ${src_dir}analyze_tcr.py --info $run_info --step blast_count' + '\n' +                           'wait\n' +                           'echo blast count completed\necho\necho ____________________\necho\necho start separate' + '\n' +                           'python3 ${src_dir}print_description.py --info $run_info --step separate' + '\n' +                           'env_parallel \'separate_fastq_basename=`awk "NR=={}" $blast_fastq_list_path | awk \'\\\'\'{print $1}\'\\\'\'` && python3 ${src_dir}separate_fastq.py --run_dir $run_dir --fastq_dir $fastq_dir --fastq_basename $separate_fastq_basename --umi_exist $umi_exist --log_path $separate_log_file --sge_task_id {} &> ${separate_log_dir}${separate_fastq_basename}.out\' ::: $sample_range' + '\n' +                           'wait' + '\n' +                           'echo separate completed\necho\necho ____________________\necho\necho start mixcr' + '\n' +                           'python3 ${src_dir}print_description.py --info $run_info --step mixcr' + '\n' +                           'mixcr_sample_range=`python ${src_dir}get_parallel_range.py --fastq_list_path $mixcr_fastq_list_path`\n' +                           'env_parallel \'mixcr_fastq_basename=`awk "NR=={}" $mixcr_fastq_list_path | awk \'\\\'\'{print $1}\'\\\'\'` && loci=`awk "NR=={}" $mixcr_fastq_list_path | awk \'\\\'\'{print $2}\'\\\'\'` && echo $mixcr_fastq_basename && bash ${src_dir}mixcr.sh "$mixcr_fastq_dir" "$mixcr_fastq_basename" "$run_dir" "$loci" "$mixcr_log_file" {} "$MIXCR_JAR" &> ${mixcr_log_dir}${mixcr_fastq_basename}.out\'' + ' ::: $mixcr_sample_range' + '\n' +                           'wait' + '\n' +                           'echo mixcr completed\necho\necho ____________________\necho\necho start merge_TRBV' + '\n' +                           'python3 ${src_dir}print_description.py --info $run_info --step merge_TRBV' + '\n' +                           'env_parallel \'merge_TRBV_fastq_basename=`awk "NR=={}" $mixcr_fastq_list_path | awk \'\\\'\'{print $1}\'\\\'\'` && loci=`awk "NR=={}" $mixcr_fastq_list_path | awk \'\\\'\'{print $2}\'\\\'\'` && echo $merge_TRBV_fastq_basename && python3 ${src_dir}merge_TRBV.py --run_dir $run_dir --fastq_basename $merge_TRBV_fastq_basename --log_path $merge_TRBV_log_file --sge_task_id {} --loci $loci &> ${mixcr_log_dir}${mixcr_fastq_basename}.out\'' + ' ::: $mixcr_sample_range' + '\n' +                           'wait' + '\n' +                           'echo merge_TRBV completed\necho\necho ____________________\necho' + '\n' +                           "if [[ $umi_exist == \"Y\" ]]; then" + "\n" +                           'echo start mixcr umi' + '\n' +                           'python3 ${src_dir}print_description.py --info $run_info --step mixcr_umi' + '\n' +                           'env_parallel \'mixcr_umi_fastq_basename=`awk "NR=={}" $mixcr_fastq_list_path | awk \'\\\'\'{print $1}\'\\\'\'` && loci=`awk "NR=={}" $mixcr_fastq_list_path | awk \'\\\'\'{print $2}\'\\\'\'` && python3 ${src_dir}count_umi.py --run_dir $run_dir --fastq_basename $mixcr_umi_fastq_basename --loci $loci --log_path $mixcr_umi_log_file --sge_task_id {} &> ${mixcr_umi_log_dir}${mixcr_umi_fastq_basename}.out\' ::: $mixcr_sample_range' + '\n' +                           'wait' + '\n' +                           'echo mixcr umi completed\necho\necho ____________________\necho' + '\n' +                           "fi" + "\n" +                           "if [[ $sc_data == \"N\" ]]; then" + "\n" +                           'echo start compare clonotype' + '\n' +                           'python3 ${src_dir}compare_clonotype.py --run_dir ' + run_dir + ' --collapse_rules_path ' + COLLAPSE_RULES_PATH + '\n' +                           'wait' + '\n' +                           'echo compare clonotype completed\necho\necho ____________________\necho\n' +                           "fi" + "\n" +                           "if [[ $sc_data == \"Y\" ]]; then" + "\n" +                           'echo start mixcr parse' + '\n' +                           'python3 ${src_dir}print_description.py --info $run_info --step mixcr_parse' + '\n' +                           'python3 ${src_dir}analyze_tcr.py --info $run_info --step mixcr_parse' + '\n' +                           'wait' + '\n' +                           'echo mixcr parse completed\necho\necho ____________________\necho\n' +                           "fi" + "\n" +                           # 'echo start aggregate' + '\n' +                           # 'python3 ${src_dir}aggregate_collapsed_counts.py --run_dir ' + run_dir + '\n' +                           # 'wait' + '\n' +                           # 'echo aggregate completed\necho\necho ____________________\necho\n' +                           'echo \'PIPELINE FINISHED :) :) :)\'\necho\n'                           )    print(master_path)# def generateWrapper(program, run_name, fastq_dir, run_dir, target_gene_exist, umi_exist, fastq_list_path):#     # count fastq files#     with open(fastq_list_path) as fastq_list_file:#         fastq_file_count = sum(1 for row in fastq_list_file)#     fastq_list_file.close()#     # generate wrapper to run scripts#     log_dir = os.path.join(run_dir, "logs",  program + "_" + today)#     os.makedirs(log_dir, exist_ok = True)#     wrapper_path = os.path.join(log_dir, program + "_wrapper.sh")#     with open(wrapper_path, "w") as wrapper_file:#         wrapper_file.write("#!/bin/bash" + "\n" +#                            "#$ -N " + program + "_" + run_name + "\n" +#                            "#$ -j y -o " + os.path.join(log_dir, "array_jobs.log") + "\n" +#                            "#$ -t 1-" + str(fastq_file_count) + "\n" +#                            "#$ -tc " + str(fastq_file_count) + "\n" +#                            "#$ -l h_vmem=25g" + "\n" +#                            "#$ -l h_rt=24:00:00" + "\n" +#                            "\n" +#                            "### define parameters" + "\n" +#                            "\n" +#                            "# batch job parameters" + "\n" +#                            "fastq_dir=" + fastq_dir + "/" + "\n" +#                            "run_dir=" + run_dir + "/" + "\n" +#                            "target_gene_exist=" + target_gene_exist + "\n" +#                            "umi_exist=" + umi_exist + "\n" +#                            "log_dir=" + log_dir + "/" + "\n\n" +#                            "log_file=${log_dir}array_jobs.log" + "\n" +#                            "if [[ $SGE_TASK_ID == 1 ]] || [[ ! -e $log_file ]]; then" + "\n" +#                            "    echo -e \"sge_no\tsample_id\tstart_time\tend_time\telapsed_time\tstatus\" > $log_file" + "\n" +#                            "fi" + "\n" +#                            "\n" +#                            "# sample arguments" + "\n" +#                            "fastq_list_path=" + fastq_list_path + "\n" +#                            "\n")#         if program == "blast":#             wrapper_file.write("fastq_basename=`awk \"NR==$SGE_TASK_ID\" $fastq_list_path | awk '{print $1}'`" + "\n" +#                                "sample_log_file=${log_dir}${fastq_basename}.out" + "\n" +#                                "\n" +#                                "BLAST_DATABASE_DIR=" + BLAST_DATABASE_DIR + "\n" +#                                "### run array job" + "\n" +#                                "bash " + SRC_DIR + "blast.sh \"$fastq_dir\" \"$fastq_basename\" \"$run_dir\" \"$target_gene_exist\" \"$log_file\" \"$SGE_TASK_ID\" \"$BLAST_DATABASE_DIR\" &> $sample_log_file" + "\n")#         if program == "blast_parse":#             wrapper_file.write("fastq_basename=`awk \"NR==$SGE_TASK_ID\" $fastq_list_path | awk '{print $1}'`" + "\n" +#                                "sample_log_file=${log_dir}${fastq_basename}.out" + "\n" +#                                "\n" +#                                "### run array job" + "\n" +#                                "source /broad/software/scripts/useuse" + "\n" +#                                "reuse -q .anaconda3-5.0.1" + "\n" +#                                "python " + SRC_DIR + "parse_blast_results.py --run_dir $run_dir --fastq_basename $fastq_basename --target_gene_exist $target_gene_exist --umi_exist $umi_exist --log_path $log_file --sge_task_id $SGE_TASK_ID &> $sample_log_file" + "\n")#         if program == "separate":#             wrapper_file.write("fastq_basename=`awk \"NR==$SGE_TASK_ID\" $fastq_list_path | awk '{print $1}'`" + "\n" +#                                "sample_log_file=${log_dir}${fastq_basename}.out" + "\n" +#                                "\n" +#                                "### run array job" + "\n" +#                                "source /broad/software/scripts/useuse" + "\n" +#                                "reuse -q .anaconda3-5.0.1" + "\n" +#                                "python " + SRC_DIR + "separate_fastq.py --run_dir $run_dir --fastq_dir $fastq_dir --fastq_basename $fastq_basename --umi_exist $umi_exist --log_path $log_file --sge_task_id $SGE_TASK_ID &> $sample_log_file" + "\n")#         if program == "mixcr":#             wrapper_file.write("fastq_basename=`awk \"NR==$SGE_TASK_ID\" $fastq_list_path | awk '{print $1}'`" + "\n" +#                                "loci=`awk \"NR==$SGE_TASK_ID\" $fastq_list_path | awk '{print $2}'`" + "\n" +#                                "sample_log_file=${log_dir}${fastq_basename}_${loci}.out" + "\n" +#                                # "merge_TRBV_sample_log_file=${merge_TRBV_log_dir}${fastq_basename}_${loci}.out"#                                "\n" +#                                "MIXCR_JAR=" + MIXCR_JAR + "\n" +#                                "### run array job" + "\n" +#                                "bash " + SRC_DIR + "mixcr.sh \"$fastq_dir\" \"$fastq_basename\" \"$run_dir\" \"$loci\" \"$log_file\" \"$SGE_TASK_ID\" &> $sample_log_file")#         if program == "merge_TRBV":#             wrapper_file.write("fastq_basename=`awk \"NR==$SGE_TASK_ID\" $fastq_list_path | awk '{print $1}'`" + "\n" +#                                "loci=`awk \"NR==$SGE_TASK_ID\" $fastq_list_path | awk '{print $2}'`" + "\n" +#                                "sample_log_file=${log_dir}${fastq_basename}_${loci}.out" + "\n" +#                                "### run array job" + "\n" +#                                "source /broad/software/scripts/useuse" + "\n" +#                                "reuse -q .anaconda3-5.0.1" + "\n" +#                                "python " + SRC_DIR + "merge_TRBV.py --run_dir $run_dir --fastq_basename $fastq_basename --log_path $log_file --sge_task_id $SGE_TASK_ID --loci $loci &> $sample_log_file")#         if program == "mixcr_umi":#             wrapper_file.write("fastq_basename=`awk \"NR==$SGE_TASK_ID\" $fastq_list_path | awk '{print $1}'`" + "\n" +#                                "loci=`awk \"NR==$SGE_TASK_ID\" $fastq_list_path | awk '{print $2}'`" + "\n" +#                                "sample_log_file=${log_dir}${fastq_basename}_${loci}.out" + "\n" +#                                "\n" +#                                "### run array job" + "\n" +#                                "source /broad/software/scripts/useuse" + "\n" +#                                "reuse -q .anaconda3-5.0.1" + "\n" +#                                "python " + SRC_DIR + "count_umi.py --run_dir $run_dir --fastq_basename $fastq_basename --loci $loci --log_path $log_file --sge_task_id $SGE_TASK_ID &> $sample_log_file" + "\n")        #         if program == "mixcr_other":#             wrapper_file.write("fastq_basename=`awk \"NR==$SGE_TASK_ID\" $fastq_list_path | awk '{print $1}'`" + "\n" +#                                "loci=`awk \"NR==$SGE_TASK_ID\" $fastq_list_path | awk '{print $2}'`" + "\n" +#                                "sample_log_file=${log_dir}${fastq_basename}_${loci}.out" + "\n" +#                                "\n" +#                                "### run array job" + "\n" +#                                "bash " + SRC_DIR + "mixcr_other.sh \"$fastq_dir\" \"$fastq_basename\" \"$run_dir\" \"$loci\" \"$log_file\" \"$SGE_TASK_ID\" &> $sample_log_file" + "\n")        #     wrapper_file.close()#     print(wrapper_path + "\n")def countBlastResults(run_dir, fastq_list, fastq_dict, target_gene_exist):    # populate well list by row and column    well2fastq_basename = {}    well_row_set = set()    well_col_set = set()    for fastq_basename in fastq_list:        well = fastq_dict[fastq_basename]["well"]        well2fastq_basename[well] = fastq_basename        well_row_set.add(well[0:1])        well_col_set.add(well[1:])    # read target gene list    gene_list = ["total", "TRA", "TRB"]    if target_gene_exist == "Y":        gene_list.append("target_gene")        target_gene_list = []        with open(TARGET_GENE_LIST_PATH) as target_gene_list_file:            target_gene_data = pandas.read_table(target_gene_list_file, sep="\t", header=0)            target_gene_list.extend(target_gene_data["name"].tolist())        target_gene_list_file.close()    # read count for each well    count_dict = nested_dict()    for fastq_basename in fastq_list:        count_dict[fastq_basename]["total"] = fastq_dict[fastq_basename]["read_count"]        count_path = os.path.join(run_dir, "blast", fastq_basename, "fastq2tcr_count.txt")        with open(count_path) as count_file:            count_data = pandas.read_table(count_file, sep = "\t", header = 0)            for index, row in count_data.iterrows():                count_dict[fastq_basename][row["loci"]] = row["read_count"]        count_file.close()        if target_gene_exist == "Y":            count_dict[fastq_basename]["target_gene"] = 0            count_path = os.path.join(run_dir, "blast", fastq_basename, "fastq2tg_count.txt")            with open(count_path) as count_file:                count_data = pandas.read_table(count_file, sep="\t", header=0)                for index, row in count_data.iterrows():                    count_dict[fastq_basename][row["target_gene"]] = row["read_count"]                    count_dict[fastq_basename]["target_gene"] += row["read_count"]            count_file.close()    # print read count list for each tcr allele (and each target gene)    gene_list_print = gene_list    if target_gene_exist == "Y":        gene_list_print.extend(target_gene_list)    read_count_path = os.path.join(run_dir, "results", "read_count.list")    with open(read_count_path, "w") as read_count_file:        read_count_file.write("well" + "\t" + "\t".join(gene for gene in gene_list_print))        for fastq_basename in fastq_list:            well = fastq_dict[fastq_basename]["well"]            read_count_file.write("\n" + well)            for gene in gene_list_print:                if gene not in count_dict[fastq_basename]:                    count_dict[fastq_basename][gene] = 0                read_count_file.write("\t" + str(count_dict[fastq_basename][gene]))    read_count_file.close()    # print read count/percentage table for tcr allele (and all target gene)    read_count_path = os.path.join(run_dir, "results", "read_count.table")    with open(read_count_path, "w") as read_count_file:        for gene in gene_list:            read_count_file.write("\n\n" +                                  gene + "\n" +                                  "\t" + "\t".join(well_col for well_col in sorted(well_col_set, key=int)))            for well_row in sorted(well_row_set):                read_count_file.write("\n" + well_row)                for well_col in sorted(well_col_set, key=int):                    if well_row + well_col in well2fastq_basename:                        fastq_basename = well2fastq_basename[well_row + well_col]                        if gene == "total":                            read_count_file.write("\t" + str(count_dict[fastq_basename]["total"]))                        else:                            read_count_file.write("\t" + str(count_dict[fastq_basename][gene] / count_dict[fastq_basename]["total"]))                    else:                        read_count_file.write("\t" + "0")    read_count_file.close()    # print list of fastq files    fastq_list_path = os.path.join(run_dir, "results", "fastq_basename4mixcr.list")    with open(fastq_list_path, "w") as fastq_list_file:        for fastq_basename in fastq_list:            for loci in ["TRA", "TRB"]:                if count_dict[fastq_basename][loci] > 0:                    fastq_list_file.write("\t".join([fastq_basename, loci]) + "\n")    fastq_list_file.close()def parseMixcrResults(run_dir, fastq_list, fastq_dict, umi_exist):    stat_file1 = os.path.join(run_dir, "results", "collapse_stat_all.txt")    sf1 = open(stat_file1, "w")    stat_file2 = os.path.join(run_dir, "results", "clone_stat_all.txt")    sf2 = open(stat_file2, "w")    sf2.write("fastq_basename,loci,number clones after collapse,percent clones collapsed")    # map well to fastq    well2fastq_basename = {}    for fastq_basename in fastq_list:        well = fastq_dict[fastq_basename]["well"]        well2fastq_basename[well] = fastq_basename    # read read count list    fastq_basename2read_count = nested_dict()    read_count_path = os.path.join(run_dir, "results", "read_count.list")    with open(read_count_path) as read_count_file:        read_count_data = pandas.read_table(read_count_file, sep = "\t", header = 0)        for index, row in read_count_data.iterrows():            fastq_basename = well2fastq_basename[row["well"]]            fastq_basename2read_count[fastq_basename]["total"] = row["total"]            fastq_basename2read_count[fastq_basename]["TRA"] = row["TRA"]            fastq_basename2read_count[fastq_basename]["TRB"] = row["TRB"]    read_count_file.close()    # read mixcr results    clone_list = defaultdict(lambda: defaultdict(list))    clone_top_list = nested_dict()    cdr3_top_dict = nested_dict()    cdr3_all_dict = nested_dict()    for fastq_basename in fastq_list:        print(fastq_basename)        well = fastq_dict[fastq_basename]["well"]        for loci in ("TRA", "TRB"):            clone_path = os.path.join(run_dir, "mixcr", fastq_basename + "_" + loci, "clones.txt")            if not os.path.exists(clone_path) or os.stat(clone_path).st_size == 0:                continue            # get unique umi count            umi_dict = {}            clone_count_dict = {}            clone_freq_dict = {}            if umi_exist == "Y":                umi_path = os.path.join(run_dir, "mixcr", fastq_basename + "_" + loci, "clones_umi.txt")                with open(umi_path) as umi_file:                    umi_data = pandas.read_table(umi_file, sep = "\t", header = 0)                    for index, row in umi_data.iterrows():                        umi_dict[row["cloneId"]] = row["umi_count"]                        clone_count_dict[row["cloneId"]] = row["clone_count"]                        clone_freq_dict[row["cloneId"]] = row["clone_fraction"]                umi_file.close()            # read clonotype            fastq_basename2read_count[fastq_basename][loci + "_cdr3"] = 0            clone_perc_sum = 0            clones = 0            same_vj_hits = defaultdict(list)            sf1.write("Result for %s at loci %s\n"%(fastq_basename, loci))            with open(clone_path) as clone_file:                clone_data = pandas.read_table(clone_file, sep = "\t", header = 0, keep_default_na = False)                for index, row in clone_data.iterrows():                    clones += 1                    # get basic counts for this clone                    clone_count = row["cloneCount"]                    clone_fraction = row["cloneFraction"]                    # get total cdr3 count for all clones                    if fastq_basename2read_count[fastq_basename][loci + "_cdr3"] == 0:                        fastq_basename2read_count[fastq_basename][loci + "_cdr3"] = int(clone_count / clone_fraction)                    # get one allele per v/j/c region                    v_hit = re.search("(%s.+?)\*" % loci, row["allVHitsWithScore"]).group(1)                    j_hit = re.search("(%s.+?)\*" % loci, row["allJHitsWithScore"]).group(1)                    vj_hit = "%s_%s"%(v_hit,j_hit)                    c_hit = ""                    if row["allCHitsWithScore"]:                        c_hit = re.search("(%s.+?)\*" % loci, row["allCHitsWithScore"]).group(1)                    # get sequences                    clone_sequence = row["clonalSequence"]                    cdr3 = row["aaSeqCDR3"]                    # Check if clone is collapsed or not based on UMI if UMIs exsits                    if umi_exist == "Y":                        if cdr3 not in umi_dict:                            continue                        else:                            umi_count = umi_dict[cdr3]                            clone_count = clone_count_dict[cdr3]                            clone_fraction = clone_freq_dict[cdr3]                            clone_list[fastq_basename][loci].append({"clone_count": clone_count,                                                   "clone_fraction": clone_fraction,                                                   "clone_sequence": clone_sequence,                                                   "v_hit": v_hit, "j_hit": j_hit, "c_hit": c_hit,                                                   "cdr3": cdr3,                                                   "umi_count": umi_count,                                                   "summary": "{percent:.2%}".format(percent=clone_fraction) + "(" + v_hit + "," + j_hit + "," + cdr3 + ")"})                    else:                        umi_count = "N/A"                        same_vj_hits[vj_hit].append({"clone_count": clone_count,                                "clone_fraction": clone_fraction,                                "clone_sequence": clone_sequence,                                "v_hit": v_hit, "j_hit": j_hit, "c_hit": c_hit,                                "cdr3": cdr3,                                "umi_count": umi_count,                                "summary": "{percent:.2%}".format(percent=clone_fraction) + "(" + v_hit + "," + j_hit + "," + cdr3 + ")"})                        # stop when the clonotype with read count is lower than cutoff                    if clone_count < CLONE_COUNT_CUTOFF:                        continue                    # stop when clonotype percentage sum is above the cutoff                    clone_perc_sum = clone_perc_sum + clone_fraction                    if clone_perc_sum > CLONE_PERC_SUM_CUTOFF:                        continue                    # only productive clonotypes                    if re.search("\_|\*", cdr3):                        continue                    cdr3_all_dict[loci][cdr3][well] = clone_fraction                        # Compare and collaps clonal types based on similiarty in CDR3 if they are not collasped based on UMIs earlier            if umi_exist == "N":                collapse = 0                collapsed_clone_list = []                for vj_combo in same_vj_hits:                    if len(same_vj_hits[vj_combo]) > 1:                        cdr3_hits_all = []                        cdr3_hits_unique = []                        tmp_clone_list = []                        umi_counts_tmp = []                        for hit in same_vj_hits[vj_combo]:                            cdr3_hit = hit["clone_sequence"]                            match = 0                            cdr3_hits_all.append(cdr3_hit)                                                        for old_hit_cdr3 in cdr3_hits_unique:                                alignment_score = pairwise2.align.globalxx(cdr3_hit, old_hit_cdr3, score_only = True)                                                                if len(cdr3_hit) >= len(old_hit_cdr3):                                    identity = (alignment_score/len(cdr3_hit))                                else:                                    identity = (alignment_score/len(old_hit_cdr3))                                                                if identity >= collapse_identity:                                    match = 1                                    collapse += 1                                                                        old_hit = same_vj_hits[vj_combo][cdr3_hits_all.index(old_hit_cdr3)]                                    write_clone_collapse(old_hit, hit, sf1)                                                                        tmp_clone_list[cdr3_hits_unique.index(old_hit_cdr3)]["clone_count"] = float(old_hit["clone_count"]) + float(hit["clone_count"])                                    tmp_clone_list[cdr3_hits_unique.index(old_hit_cdr3)]["clone_fraction"] = float(old_hit["clone_fraction"]) + float(hit["clone_fraction"])                                    break                            if match == 0:                                cdr3_hits_unique.append(cdr3_hit)                                tmp_clone_list.append(hit)                                                       collapsed_clone_list.extend(tmp_clone_list)                    else:                        hit = same_vj_hits[vj_combo][0]                        collapsed_clone_list.append(hit)                                        # Write stat files                if clones != 0:                    sf2.write("%s,%s,%d,%0.2f\n"%(fastq_basename, loci, len(collapsed_clone_list), (int(collapse)/float(clones))*100))                    sf1.write("Total number of clones before collapse:%d\n"%(clones))                    sf1.write("Number of clones collapsed:%d\n\n"%collapse)                clone_list[fastq_basename][loci] = collapsed_clone_list                        # Sort clone list by fraction            newlist = sorted(clone_list[fastq_basename][loci], key=lambda k: k['clone_fraction'], reverse=True)            clone_list[fastq_basename][loci] = newlist                        # Save top one hit            if loci not in clone_top_list[fastq_basename] and len(clone_list[fastq_basename][loci]) != 0:                if re.search("\_|\*", clone_list[fastq_basename][loci][0]["cdr3"]):                    if len(clone_list[fastq_basename][loci]) >= 2:                        top_id = 1                        top1_hit = clone_list[fastq_basename][loci][1]                        cdr3 = clone_list[fastq_basename][loci][1]["cdr3"]                    else:                        continue                else:                    top_id = 0                    top1_hit = clone_list[fastq_basename][loci][0]                    cdr3 = clone_list[fastq_basename][loci][0]["cdr3"]                clone_sequence = top1_hit["clone_sequence"]                clone_fraction = top1_hit["clone_fraction"]                fastq_path = os.path.join(run_dir, "fastq", "tcr",                                          fastq_basename + "_" + loci + "_L001_R1_001.fastq")                flank_sequence, flank_count, flank_fraction = getFlankSequence(fastq_path, clone_sequence, 20,                                                                               50)                clone_top_list[fastq_basename][loci] = {"top_id": top_id,                                                        "flank_sequence": flank_sequence,                                                        "flank_count": flank_count,                                                        "flank_fraction": flank_fraction}                cdr3_top_dict[loci][cdr3][well] = clone_fraction                if ("TRA" in clone_top_list[fastq_basename].keys()) & ("TRB" in clone_top_list[fastq_basename].keys()):            cdr3_tra = clone_list[fastq_basename]["TRA"][clone_top_list[fastq_basename]["TRA"]["top_id"]]["cdr3"]            cdr3_trb = clone_list[fastq_basename]["TRB"][clone_top_list[fastq_basename]["TRB"]["top_id"]]["cdr3"]            cdr3_top_dict["paired"][cdr3_tra + "\t" + cdr3_trb][well] = ""            # parse mixcr results    # top 1 clonotype per well    mixcr_path = os.path.join(run_dir, "results", "mixcr_clonotype_per_well.list")    with open(mixcr_path, "w") as mixcr_file:        mixcr_file.write("\t".join(            ["well", "loci", "tcr_read_count", "cdr3_read_count", "cdr3_fraction_list", "v", "j", "c",             "clonal_sequences", "cdr3", "cdr3_read_count", "cdr3_fraction", "flank_sequence", "flank_count",             "flank_fraction"]))        for fastq_basename in fastq_list:            well = fastq_dict[fastq_basename]["well"]            for loci in ("TRA", "TRB"):                mixcr_file.write("\n" + "\t".join([well, loci, str(fastq_basename2read_count[fastq_basename][loci]),                                                   str(fastq_basename2read_count[fastq_basename][loci + "_cdr3"])]))                cdr3_fraction_list = []                if len(clone_list[fastq_basename][loci]) != 0:                    for clone_n in (0, 1, 2):                        if clone_n + 1 > len(clone_list[fastq_basename][loci]):                            break                        cdr3_fraction_list.append(clone_list[fastq_basename][loci][clone_n]["summary"])                    mixcr_file.write("\t" + ",".join(cdr3_fraction_list))                else:                    mixcr_file.write("\tNo clones identified")                if loci in clone_top_list[fastq_basename].keys():                    clone_top_id = clone_top_list[fastq_basename][loci]["top_id"]                    mixcr_file.write("\t" + "\t".join([clone_list[fastq_basename][loci][clone_top_id]["v_hit"],                                                       clone_list[fastq_basename][loci][clone_top_id]["j_hit"],                                                       clone_list[fastq_basename][loci][clone_top_id]["c_hit"],                                                       clone_list[fastq_basename][loci][clone_top_id]["clone_sequence"],                                                       clone_list[fastq_basename][loci][clone_top_id]["cdr3"], str(                            clone_list[fastq_basename][loci][clone_top_id]["clone_count"]), str(                            clone_list[fastq_basename][loci][clone_top_id]["clone_fraction"]),                                                       clone_top_list[fastq_basename][loci]["flank_sequence"],                                                       str(clone_top_list[fastq_basename][loci]["flank_count"]),                                                       str(clone_top_list[fastq_basename][loci]["flank_fraction"])]))    mixcr_file.close()    ## top 1 clonotype across per well    for loci in ("TRA", "TRB"):        mixcr_path = os.path.join(run_dir, "results", "mixcr_clonotype_" + loci + "_across_well.table")        with open(mixcr_path, "w") as mixcr_file:            mixcr_file.write("\t".join(["well", "CDR3", "clone_fraction"]) + "\t" + "\t".join(                fastq_dict[fastq_basename]["well"] for fastq_basename in fastq_list))            for fastq_basename in fastq_list:                well = fastq_dict[fastq_basename]["well"]                if loci not in clone_top_list[fastq_basename].keys():  # no tcr separated or no cdr3 identified                    mixcr_file.write("\n" + "\t".join([well, "-", "-"]))                else:                    clone_top_id = clone_top_list[fastq_basename][loci]["top_id"]                    cdr3 = clone_list[fastq_basename][loci][clone_top_id]["cdr3"]                    fraction = clone_list[fastq_basename][loci][clone_top_id]["clone_fraction"]                    mixcr_file.write("\n" + "\t".join([well, cdr3, str(fraction)]))                    for fastq_basename2 in fastq_list:                        if loci not in clone_top_list[fastq_basename2].keys():                            mixcr_file.write("\t" + "")                        else:                            well2 = fastq_dict[fastq_basename2]["well"]                            clone_top_id2 = clone_top_list[fastq_basename2][loci]["top_id"]                            cdr32 = clone_list[fastq_basename2][loci][clone_top_id2]["cdr3"]                            if well2 not in cdr3_all_dict[loci][cdr3]:                                fraction = 0                            else:                                fraction = cdr3_all_dict[loci][cdr3][well2]                            if cdr32 == cdr3:                                fraction = "-"                            mixcr_file.write("\t" + str(fraction))        mixcr_file.close()    ## cdr3 recurrence    cdr3_path = os.path.join(run_dir, "results", "mixcr_clonotype_recurrence.list")    with open(cdr3_path, "w") as cdr3_file:        for loci in ("TRA", "TRB", "paired"):            cdr3_file.write("\n\n" + "\t".join([loci, "well_list", "well_count"]))            for cdr3 in cdr3_top_dict[loci]:                cdr3_file.write("\n" + "\t".join([cdr3, ",".join(cdr3_top_dict[loci][cdr3].keys()),                                                  str(len(cdr3_top_dict[loci][cdr3]))]))    cdr3_file.close()    sf1.close()    sf2.close()def getFlankSequence(fastq_path, clone_sequence, up_len, down_len):    flank_sequence_dict = {}    flank_count_all = 0    with open(fastq_path) as fastq_file:        for count, row in enumerate(fastq_file, start = 1):            if count % 4 == 2:                s = re.search("(.{%d}%s.{%d})" % (up_len, clone_sequence, down_len), row)                if s:                    flank_sequence = s.group(1)                    flank_count_all += 1                    if flank_sequence not in flank_sequence_dict:                        flank_sequence_dict[flank_sequence] = 0                    flank_sequence_dict[flank_sequence] += 1    fastq_file.close()    if not flank_sequence_dict:        return "", "", ""    flank_count_max = max(flank_sequence_dict.values())    for flank_sequence in flank_sequence_dict:        flank_count = flank_sequence_dict[flank_sequence]        if flank_count == flank_count_max:            flank_fraction = flank_count / flank_count_all            return flank_sequence, flank_count, flank_fractiondef write_clone_collapse(old_hit, hit, sf1):    sf1.write(",".join(["clone_count", "clone_fraction", "v_hit", "j_hit", "c_hit",                    "clone_sequence", "cdr3", "umi_count", "summary"]) + "\n")    sf1.write(",".join([str(old_hit["clone_count"]),                    str(old_hit["clone_fraction"]),                    old_hit["v_hit"],                    old_hit["j_hit"],                    old_hit["c_hit"],                    old_hit["clone_sequence"],                    old_hit["cdr3"],                    str(old_hit["umi_count"]),                    old_hit["summary"]]) + "\n")    sf1.write(",".join([str(hit["clone_count"]),                    str(hit["clone_fraction"]),                    hit["v_hit"],                    hit["j_hit"],                    hit["c_hit"],                    hit["clone_sequence"],                    hit["cdr3"],                    str(hit["umi_count"]),                    hit["summary"]]) + "\n")if __name__ == "__main__": main()